"""
analyze_keywords.py â€” Î’ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î· Î­ÎºÎ´Î¿ÏƒÎ·
---------------------------------------
Î‘Î½Î±Î»ÏÎµÎ¹ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï„Ï‰Î½ Î¿Î¼Î¹Î»Î¹ÏÎ½ Ï„Î·Ï‚ Î’Î¿Ï…Î»Î®Ï‚:
1. Keywords Î±Î½Î¬ Î¿Î¼Î¹Î»Î¯Î±
2. Keywords Î±Î½Î¬ Î²Î¿Ï…Î»ÎµÏ…Ï„Î®
3. Keywords Î±Î½Î¬ ÎºÏŒÎ¼Î¼Î±
4. Keywords Î±Î½Î¬ Î­Ï„Î¿Ï‚ (ÏƒÏ‡ÎµÏ„Î¹ÎºÎ¬ Î¼Îµ ÎºÏŒÎ¼Î¼Î± & Î²Î¿Ï…Î»ÎµÏ…Ï„Î®)
"""

from elasticsearch import Elasticsearch
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import text
import pandas as pd
import re
import unicodedata
from datetime import datetime
from tqdm import tqdm
import os

import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="sklearn.feature_extraction.text")

# --- Î”Î¹Î±Î³ÏÎ±Ï†Î® Ï€Î±Î»Î¹ÏÎ½ pkl ---
for f in ["party_keywords.pkl", "member_keywords.pkl", "speech_keywords.pkl",
          "yearly_party_keywords.pkl", "yearly_member_keywords.pkl", "yearly_keywords.pkl"]:
    if os.path.exists(f):
        os.remove(f)
        print(f"ğŸ§¹ Î”Î¹Î±Î³ÏÎ¬Ï†Î·ÎºÎµ Ï€Î±Î»Î¹ÏŒ Î±ÏÏ‡ÎµÎ¯Î¿: {f}")

# -----------------------------------------------------------
# 1. Î£ÏÎ½Î´ÎµÏƒÎ· Î¼Îµ Elasticsearch
# -----------------------------------------------------------
es = Elasticsearch(
    [{"host": "localhost", "port": 9200, "scheme": "http"}],
    verify_certs=False,
    ssl_show_warn=False,
    request_timeout=300
)
INDEX_NAME = "greek_parliament_speeches"

# -----------------------------------------------------------
# 2. ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…
# -----------------------------------------------------------

# --- Î¦ÏŒÏÏ„Ï‰ÏƒÎ· stopwords Î±Ï€ÏŒ Î±ÏÏ‡ÎµÎ¯Î¿ ---
def load_stopwords(path):
    with open(path, "r", encoding="utf-8") as f:
        return {line.strip() for line in f if line.strip()}

stopword_path = os.path.join(os.path.dirname(__file__), "data", "stopwords-el.txt")
greek_stopwords = load_stopwords(stopword_path)

def clean_text(text: str) -> str:
    text = unicodedata.normalize("NFC", str(text))
    # ÎšÏÎ±Ï„Î¬Î¼Îµ Î¼ÏŒÎ½Î¿ ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬ Î³ÏÎ¬Î¼Î¼Î±Ï„Î± (Ï€ÎµÎ¶Î¬ ÎºÎ±Î¹ ÎºÎµÏ†Î±Î»Î±Î¯Î±, Î¼Îµ Î® Ï‡Ï‰ÏÎ¯Ï‚ Ï„ÏŒÎ½Î¿Ï…Ï‚)
    text = re.sub(r"[^Î‘-Î©Î†ÎˆÎ‰ÎŠÎŒÎÎÎªÎ«Î±-Ï‰Î¬Î­Î®Î¯ÏŒÏÏÏŠÏ‹ÎÎ°\s]", " ", text)
    text = re.sub(r"\s+", " ", text)
    # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ Ï€ÎµÎ¶Î¬ ÎœÎ•Î¤Î‘ Ï„Î¿Î½ ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒ (ÏÏƒÏ„Îµ Î½Î± ÎºÏÎ±Ï„Î·Î¸Î¿ÏÎ½ Î¿Î¹ Ï„ÏŒÎ½Î¿Î¹)
    text = text.lower()
    # Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¿Ï…Î¼Îµ Î»Î­Î¾ÎµÎ¹Ï‚ Î¼Îµ Î¼Î®ÎºÎ¿Ï‚ > 2 ÎºÎ±Î¹ ÏŒÏ‡Î¹ ÏƒÏ„Î± stopwords
    tokens = [
        w for w in text.split()
        if len(w) > 2 and w not in greek_stopwords
    ]
    return " ".join(tokens)



# -----------------------------------------------------------
# 3. Î‘Î½Î¬ÎºÏ„Î·ÏƒÎ· Î¿Î¼Î¹Î»Î¹ÏÎ½
# -----------------------------------------------------------
def fetch_all_speeches(batch_size=5000):
    data = []
    res = es.search(
        index=INDEX_NAME,
        body={"query": {"match_all": {}}},
        scroll="5m",
        size=batch_size
    )
    scroll_id = res["_scroll_id"]
    total_hits = res["hits"]["total"]["value"]
    print(f"Î£ÏÎ½Î¿Î»Î¿ Î¿Î¼Î¹Î»Î¹ÏÎ½ Ï€ÏÎ¿Ï‚ Î±Î½Î¬ÎºÏ„Î·ÏƒÎ·: {total_hits}")

    fetched = 0
    hits = res["hits"]["hits"]

    while hits:
        for hit in hits:
            src = hit["_source"]
            date = src.get("date", "")
            try:
                year = datetime.strptime(date, "%d/%m/%Y").year
            except Exception:
                year = None

            data.append({
                "member_name": src.get("member_name", "").strip(),
                "party": src.get("party", "").strip(),
                "date": date,
                "year": year,
                "speech": clean_text(src.get("speech", "")),
            })

        fetched += len(hits)
        print(f"âœ… Î‘Î½Î±ÎºÏ„Î®Î¸Î·ÎºÎ±Î½ {fetched}/{total_hits} Î¿Î¼Î¹Î»Î¯ÎµÏ‚")
        res = es.scroll(scroll_id=scroll_id, scroll="5m")
        scroll_id = res["_scroll_id"]
        hits = res["hits"]["hits"]

    return pd.DataFrame(data)

# -----------------------------------------------------------
# 4. TF-IDF Î±Î½Î¬ Î¿Î¼Î¬Î´Î± (ÎºÏŒÎ¼Î¼Î±, Î²Î¿Ï…Î»ÎµÏ…Ï„Î®Ï‚)
# -----------------------------------------------------------
def compute_keywords(df: pd.DataFrame, group_col, top_n=10) -> dict:
    results = {}
    grouped = df.groupby(group_col)["speech"].apply(lambda x: " ".join(x))
    vectorizer = TfidfVectorizer(
        max_features=5000,
        stop_words=list(greek_stopwords),
        token_pattern=r"(?u)\b[Î±-Ï‰]{3,}\b"
    )

    tfidf_matrix = vectorizer.fit_transform(grouped.values)
    feature_names = vectorizer.get_feature_names_out()
    for idx, name in enumerate(grouped.index):
        scores = tfidf_matrix[idx].toarray()[0]
        top_idx = scores.argsort()[-top_n:][::-1]
        results[name] = [(feature_names[i], round(scores[i], 3)) for i in top_idx]
    return results

# -----------------------------------------------------------
# 5. TF-IDF Î±Î½Î¬ Î¿Î¼Î¹Î»Î¯Î± (batching)
# -----------------------------------------------------------
def compute_keywords_per_speech(df: pd.DataFrame, top_n=10, batch_size=5000) -> dict:
    results = {}
    vectorizer = TfidfVectorizer(
        max_features=5000,
        stop_words=list(greek_stopwords),
        token_pattern=r"(?u)\b[Î±-Ï‰]{3,}\b"
    )

    speeches = df["speech"].tolist()
    indices = df.index.tolist()
    for start in tqdm(range(0, len(speeches), batch_size), desc="Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ keywords Î±Î½Î¬ Î¿Î¼Î¹Î»Î¯Î±"):
        batch = [(i, s) for i, s in zip(indices[start:start+batch_size], speeches[start:start+batch_size]) if s.strip()]
        if not batch:
            continue
        idxs, texts = zip(*batch)
        tfidf_matrix = vectorizer.fit_transform(texts)
        feature_names = vectorizer.get_feature_names_out()
        for i, row in zip(idxs, tfidf_matrix.toarray()):
            top_idx = row.argsort()[-top_n:][::-1]
            results[i] = [(feature_names[j], round(row[j], 3)) for j in top_idx]
    return results

# -----------------------------------------------------------
# 6. TF-IDF Î±Î½Î¬ Î­Ï„Î¿Ï‚ + ÏƒÏ‡Î­ÏƒÎ· (ÎºÏŒÎ¼Î¼Î±/Î²Î¿Ï…Î»ÎµÏ…Ï„Î®Ï‚)
# -----------------------------------------------------------
def compute_keywords_over_time(df: pd.DataFrame, group_col="year", related_col=None, top_n=10) -> dict:
    results = {}
    if related_col:
        df = df.dropna(subset=[group_col, related_col])
        grouped = df.groupby([group_col, related_col])["speech"].apply(lambda x: " ".join(x))
    else:
        df = df.dropna(subset=[group_col])
        grouped = df.groupby(group_col)["speech"].apply(lambda x: " ".join(x))
    vectorizer = TfidfVectorizer(
        max_features=5000,
        stop_words=list(greek_stopwords),
        token_pattern=r"(?u)\b[Î±-Ï‰]{3,}\b"
    )

    tfidf_matrix = vectorizer.fit_transform(grouped.values)
    feature_names = vectorizer.get_feature_names_out()
    for idx, name in enumerate(grouped.index):
        scores = tfidf_matrix[idx].toarray()[0]
        top_idx = scores.argsort()[-top_n:][::-1]
        results[name] = [(feature_names[i], round(scores[i], 3)) for i in top_idx]
    return results

# -----------------------------------------------------------
# 7. ÎšÏÏÎ¹Î± ÏÎ¿Î®
# -----------------------------------------------------------
if __name__ == "__main__":
    print("ğŸ”¹ Î‘Î½Î¬ÎºÏ„Î·ÏƒÎ· Î¿Î¼Î¹Î»Î¹ÏÎ½ Î±Ï€ÏŒ Ï„Î¿Î½ Elasticsearch...")
    df = fetch_all_speeches()
    # Î ÏÎ¿ÏƒÎ¿ÏÎ¹Î½Î¬, Î³Î¹Î± Î´Î¿ÎºÎ¹Î¼Î±ÏƒÏ„Î¹ÎºÎ¿ÏÏ‚ ÏƒÎºÎ¿Ï€Î¿ÏÏ‚, Ï€ÎµÏÎ¹Î¿ÏÎ¯Î¶Î¿Ï…Î¼Îµ ÏƒÎµ 10.000 Î¿Î¼Î¹Î»Î¯ÎµÏ‚
    #df = df.sample(n=10000, random_state=42)  # Î® .head(10000)
    #print(f"ğŸ“Š Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ½Ï„Î±Î¹ {len(df)} Î¿Î¼Î¹Î»Î¯ÎµÏ‚ Î³Î¹Î± Î±Î½Î¬Î»Ï…ÏƒÎ· (Î´Î¿ÎºÎ¹Î¼Î±ÏƒÏ„Î¹ÎºÏŒ Î´ÎµÎ¯Î³Î¼Î±).")
    
    print(f"ğŸ”¸ Î‘Î½Î±ÎºÏ„Î®Î¸Î·ÎºÎ±Î½ {len(df)} Î¿Î¼Î¹Î»Î¯ÎµÏ‚")

    print("\nğŸ§  Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ keywords Î±Î½Î¬ ÎºÏŒÎ¼Î¼Î±...")
    party_keywords = compute_keywords(df, "party")

    print("\nğŸ§  Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ keywords Î±Î½Î¬ Î²Î¿Ï…Î»ÎµÏ…Ï„Î®...")
    member_keywords = compute_keywords(df, "member_name")

    print("\nğŸ“ Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ keywords Î±Î½Î¬ Î¿Î¼Î¹Î»Î¯Î±...")
    speech_keywords = compute_keywords_per_speech(df)

    print("\nğŸ“… Î‘Î½Î¬Î»Ï…ÏƒÎ· keywords Î±Î½Î¬ Î­Ï„Î¿Ï‚ Î±Î½Î¬ ÎºÏŒÎ¼Î¼Î±...")
    yearly_party_keywords = compute_keywords_over_time(df, group_col="year", related_col="party")

    print("\nğŸ“… Î‘Î½Î¬Î»Ï…ÏƒÎ· keywords Î±Î½Î¬ Î­Ï„Î¿Ï‚ Î±Î½Î¬ Î²Î¿Ï…Î»ÎµÏ…Ï„Î®...")
    yearly_member_keywords = compute_keywords_over_time(df, group_col="year", related_col="member_name")

    pd.to_pickle(party_keywords, "party_keywords.pkl")
    pd.to_pickle(member_keywords, "member_keywords.pkl")
    pd.to_pickle(speech_keywords, "speech_keywords.pkl")
    pd.to_pickle(yearly_party_keywords, "yearly_party_keywords.pkl")
    pd.to_pickle(yearly_member_keywords, "yearly_member_keywords.pkl")

    print("\nğŸ“¦ Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎ±Î½ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± ÏƒÎµ .pkl Î±ÏÏ‡ÎµÎ¯Î±")
    print("âœ… ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")