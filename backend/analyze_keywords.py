"""
analyze_keywords.py â€” Î’ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î· Î­ÎºÎ´Î¿ÏƒÎ·
---------------------------------------
Î‘Î½Î±Î»ÏÎµÎ¹ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï„Ï‰Î½ Î¿Î¼Î¹Î»Î¹ÏÎ½ Ï„Î·Ï‚ Î’Î¿Ï…Î»Î®Ï‚:
1. Keywords Î±Î½Î¬ Î¿Î¼Î¹Î»Î¯Î±
2. Keywords Î±Î½Î¬ Î²Î¿Ï…Î»ÎµÏ…Ï„Î®
3. Keywords Î±Î½Î¬ ÎºÏŒÎ¼Î¼Î±
4. Keywords Î±Î½Î¬ Î­Ï„Î¿Ï‚ (ÏƒÏ‡ÎµÏ„Î¹ÎºÎ¬ Î¼Îµ ÎºÏŒÎ¼Î¼Î± & Î²Î¿Ï…Î»ÎµÏ…Ï„Î®)
"""

from elasticsearch import Elasticsearch
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import text
import pandas as pd
import re
import unicodedata
from datetime import datetime
from tqdm import tqdm
import os

# --- Î”Î¹Î±Î³ÏÎ±Ï†Î® Ï€Î±Î»Î¹ÏÎ½ pkl ---
for f in ["party_keywords.pkl", "member_keywords.pkl", "speech_keywords.pkl",
          "yearly_party_keywords.pkl", "yearly_member_keywords.pkl", "yearly_keywords.pkl"]:
    if os.path.exists(f):
        os.remove(f)
        print(f"ğŸ§¹ Î”Î¹Î±Î³ÏÎ¬Ï†Î·ÎºÎµ Ï€Î±Î»Î¹ÏŒ Î±ÏÏ‡ÎµÎ¯Î¿: {f}")

# --- Î•Î»Î»Î·Î½Î¹ÎºÎ¬ stopwords ---
greek_stopwords = text.ENGLISH_STOP_WORDS.union({
    "ÎºÎ±Î¹", "Î½Î±", "Ï„Î¿", "Î·", "Î¿", "Î±Ï€ÏŒ", "Î¼Îµ", "ÏƒÏ„Î¿", "Ï„Î·Î½", "Ï„Î·Ï‚", "ÏƒÎµ",
    "Î¸Î±", "Ï‰Ï‚", "Î³Î¹Î±", "Ï„Ï‰Î½", "Ï„Î¹Ï‚", "Ï„Î¿Î½", "Ï„Î¿Ï…", "Ï€ÏÎ¿Ï‚", "Î´Îµ", "Î´ÎµÎ½",
    "Î±Î½", "Î¼Î¹Î±", "Î­Ï‡Ï‰", "ÎµÎ¯Î½Î±Î¹", "Î®Ï„Î±Î½", "ÏŒÏ„Î¹", "ÏŒÏ€Ï‰Ï‚", "ÎµÏ€Î¯ÏƒÎ·Ï‚", "Î±Î»Î»Î¬"
})
greek_stopwords = {unicodedata.normalize("NFC", w) for w in greek_stopwords}

# -----------------------------------------------------------
# 1. Î£ÏÎ½Î´ÎµÏƒÎ· Î¼Îµ Elasticsearch
# -----------------------------------------------------------
es = Elasticsearch(
    [{"host": "localhost", "port": 9200, "scheme": "http"}],
    verify_certs=False,
    ssl_show_warn=False,
    request_timeout=300
)
INDEX_NAME = "greek_parliament_speeches"

# -----------------------------------------------------------
# 2. ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…
# -----------------------------------------------------------
def clean_text(text: str) -> str:
    text = unicodedata.normalize("NFC", str(text))
    text = re.sub(r"[^Î‘-Î©Î±-Ï‰A-Za-z\s]", " ", text)
    text = re.sub(r"\s+", " ", text)
    tokens = [w for w in text.lower().split() if len(w) > 2 and w not in greek_stopwords and re.match(r"^[Î±-Ï‰]+$", w)]
    return " ".join(tokens)

# -----------------------------------------------------------
# 3. Î‘Î½Î¬ÎºÏ„Î·ÏƒÎ· Î¿Î¼Î¹Î»Î¹ÏÎ½
# -----------------------------------------------------------
def fetch_all_speeches(batch_size=5000):
    data = []
    res = es.search(
        index=INDEX_NAME,
        body={"query": {"match_all": {}}},
        scroll="5m",
        size=batch_size
    )
    scroll_id = res["_scroll_id"]
    total_hits = res["hits"]["total"]["value"]
    print(f"Î£ÏÎ½Î¿Î»Î¿ Î¿Î¼Î¹Î»Î¹ÏÎ½ Ï€ÏÎ¿Ï‚ Î±Î½Î¬ÎºÏ„Î·ÏƒÎ·: {total_hits}")

    fetched = 0
    hits = res["hits"]["hits"]

    while hits:
        for hit in hits:
            src = hit["_source"]
            date = src.get("date", "")
            try:
                year = datetime.strptime(date, "%d/%m/%Y").year
            except Exception:
                year = None

            data.append({
                "member_name": src.get("member_name", "").strip(),
                "party": src.get("party", "").strip(),
                "date": date,
                "year": year,
                "speech": clean_text(src.get("speech", "")),
            })

        fetched += len(hits)
        print(f"âœ… Î‘Î½Î±ÎºÏ„Î®Î¸Î·ÎºÎ±Î½ {fetched}/{total_hits} Î¿Î¼Î¹Î»Î¯ÎµÏ‚")
        res = es.scroll(scroll_id=scroll_id, scroll="5m")
        scroll_id = res["_scroll_id"]
        hits = res["hits"]["hits"]

    return pd.DataFrame(data)

# -----------------------------------------------------------
# 4. TF-IDF Î±Î½Î¬ Î¿Î¼Î¬Î´Î± (ÎºÏŒÎ¼Î¼Î±, Î²Î¿Ï…Î»ÎµÏ…Ï„Î®Ï‚)
# -----------------------------------------------------------
def compute_keywords(df: pd.DataFrame, group_col, top_n=10) -> dict:
    results = {}
    grouped = df.groupby(group_col)["speech"].apply(lambda x: " ".join(x))
    vectorizer = TfidfVectorizer(
        max_features=5000,
        stop_words=list(greek_stopwords),
        token_pattern=r"(?u)\b[Î±-Ï‰]{3,}\b"
    )
    tfidf_matrix = vectorizer.fit_transform(grouped.values)
    feature_names = vectorizer.get_feature_names_out()
    for idx, name in enumerate(grouped.index):
        scores = tfidf_matrix[idx].toarray()[0]
        top_idx = scores.argsort()[-top_n:][::-1]
        results[name] = [(feature_names[i], round(scores[i], 3)) for i in top_idx]
    return results

# -----------------------------------------------------------
# 5. TF-IDF Î±Î½Î¬ Î¿Î¼Î¹Î»Î¯Î± (batching)
# -----------------------------------------------------------
def compute_keywords_per_speech(df: pd.DataFrame, top_n=10, batch_size=5000) -> dict:
    results = {}
    vectorizer = TfidfVectorizer(
        max_features=5000,
        stop_words=list(greek_stopwords),
        token_pattern=r"(?u)\b[Î±-Ï‰]{3,}\b"
    )
    speeches = df["speech"].tolist()
    indices = df.index.tolist()
    for start in tqdm(range(0, len(speeches), batch_size), desc="Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ keywords Î±Î½Î¬ Î¿Î¼Î¹Î»Î¯Î±"):
        batch = [(i, s) for i, s in zip(indices[start:start+batch_size], speeches[start:start+batch_size]) if s.strip()]
        if not batch:
            continue
        idxs, texts = zip(*batch)
        tfidf_matrix = vectorizer.fit_transform(texts)
        feature_names = vectorizer.get_feature_names_out()
        for i, row in zip(idxs, tfidf_matrix.toarray()):
            top_idx = row.argsort()[-top_n:][::-1]
            results[i] = [(feature_names[j], round(row[j], 3)) for j in top_idx]
    return results

# -----------------------------------------------------------
# 6. TF-IDF Î±Î½Î¬ Î­Ï„Î¿Ï‚ + ÏƒÏ‡Î­ÏƒÎ· (ÎºÏŒÎ¼Î¼Î±/Î²Î¿Ï…Î»ÎµÏ…Ï„Î®Ï‚)
# -----------------------------------------------------------
def compute_keywords_over_time(df: pd.DataFrame, group_col="year", related_col=None, top_n=10) -> dict:
    results = {}
    if related_col:
        df = df.dropna(subset=[group_col, related_col])
        grouped = df.groupby([group_col, related_col])["speech"].apply(lambda x: " ".join(x))
    else:
        df = df.dropna(subset=[group_col])
        grouped = df.groupby(group_col)["speech"].apply(lambda x: " ".join(x))
    vectorizer = TfidfVectorizer(
        max_features=5000,
        stop_words=list(greek_stopwords),
        token_pattern=r"(?u)\b[Î±-Ï‰]{3,}\b"
    )
    tfidf_matrix = vectorizer.fit_transform(grouped.values)
    feature_names = vectorizer.get_feature_names_out()
    for idx, name in enumerate(grouped.index):
        scores = tfidf_matrix[idx].toarray()[0]
        top_idx = scores.argsort()[-top_n:][::-1]
        results[name] = [(feature_names[i], round(scores[i], 3)) for i in top_idx]
    return results

# -----------------------------------------------------------
# 7. ÎšÏÏÎ¹Î± ÏÎ¿Î®
# -----------------------------------------------------------
if __name__ == "__main__":
    print("ğŸ”¹ Î‘Î½Î¬ÎºÏ„Î·ÏƒÎ· Î¿Î¼Î¹Î»Î¹ÏÎ½ Î±Ï€ÏŒ Ï„Î¿Î½ Elasticsearch...")
    df = fetch_all_speeches()
    print(f"ğŸ”¸ Î‘Î½Î±ÎºÏ„Î®Î¸Î·ÎºÎ±Î½ {len(df)} Î¿Î¼Î¹Î»Î¯ÎµÏ‚")

    print("\nğŸ§  Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ keywords Î±Î½Î¬ ÎºÏŒÎ¼Î¼Î±...")
    party_keywords = compute_keywords(df, "party")

    print("\nğŸ§  Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ keywords Î±Î½Î¬ Î²Î¿Ï…Î»ÎµÏ…Ï„Î®...")
    member_keywords = compute_keywords(df, "member_name")

    print("\nğŸ“ Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ keywords Î±Î½Î¬ Î¿Î¼Î¹Î»Î¯Î±...")
    speech_keywords = compute_keywords_per_speech(df)

    print("\nğŸ“… Î‘Î½Î¬Î»Ï…ÏƒÎ· keywords Î±Î½Î¬ Î­Ï„Î¿Ï‚ Î±Î½Î¬ ÎºÏŒÎ¼Î¼Î±...")
    yearly_party_keywords = compute_keywords_over_time(df, group_col="year", related_col="party")

    print("\nğŸ“… Î‘Î½Î¬Î»Ï…ÏƒÎ· keywords Î±Î½Î¬ Î­Ï„Î¿Ï‚ Î±Î½Î¬ Î²Î¿Ï…Î»ÎµÏ…Ï„Î®...")
    yearly_member_keywords = compute_keywords_over_time(df, group_col="year", related_col="member_name")

    pd.to_pickle(party_keywords, "party_keywords.pkl")
    pd.to_pickle(member_keywords, "member_keywords.pkl")
    pd.to_pickle(speech_keywords, "speech_keywords.pkl")
    pd.to_pickle(yearly_party_keywords, "yearly_party_keywords.pkl")
    pd.to_pickle(yearly_member_keywords, "yearly_member_keywords.pkl")

    print("\nğŸ“¦ Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎ±Î½ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± ÏƒÎµ .pkl Î±ÏÏ‡ÎµÎ¯Î±")
    print("âœ… ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")